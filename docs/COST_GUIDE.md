# ğŸ’° Guia de EficiÃªncia e Custo: Infinite Memory MCP

Este guia ajuda vocÃª a entender onde o **Infinite Memory** brilha e como usÃ¡-lo sem estourar seu orÃ§amento (seja em dinheiro via API ou em RAM via Ollama).

---

## ğŸš¦ Quando Usar? (Matriz de DecisÃ£o)

| CenÃ¡rio do Projeto | Tamanho Estimado | Infinite Memory (`@summary`) | EstratÃ©gia Recomendada |
| :--- | :--- | :--- | :--- |
| **Micro-ServiÃ§os / Scripts** | < 10 Arquivos | âœ… **Ideal** | Use sem medo. Custo irrelevante. Performance mÃ¡xima. |
| **Projetos MÃ©dios (MVP)** | 10 - 50 Arquivos | âœ… **Recomendado** | O ganho de produtividade supera o custo baixo de tokens. |
| **AplicaÃ§Ãµes Grandes (Monolitos)** | 50 - 200 Arquivos | âš ï¸ **Cuidado** | Use `ignore_folder` para filtrar testes e docs. NÃ£o leia a cada mensagem. |
| **Enterprise / Legacy** | > 200 Arquivos | âŒ **NÃ£o Recomendado** | Contexto excederÃ¡ o limite. Use `@tree` e leia arquivos pontuais. |

---

## ğŸ’¸ Estimativa de Custos (Cloud LLMs)

Considerando o envio _completo_ do contexto a cada nova thread (sessÃ£o) iniciada.
_Valores baseados em preÃ§os mÃ©dios de mercado (2024/2025)._

| Tamanho do Projeto | Tokens Aprox. | Custo Claude 3.5 Sonnet | Custo GPT-4o |
| :--- | :--- | :--- | :--- |
| **5.000 tokens** (Atual) | ~20 KB | ~$0.015 / chat | ~$0.025 / chat |
| **20.000 tokens** | ~80 KB | ~$0.06 / chat | ~$0.10 / chat |
| **100.000 tokens** | ~400 KB | ~$0.30 / chat | ~$0.50 / chat |
| **200.000 tokens** | ~800 KB | ~$0.60 / chat | ~$1.00 / chat |

> **Regra de Ouro:** Se o projeto custa mais de $0.10 por chat para carregar, PARE de usar o modo "Infinio" bruto e comece a filtrar.

---

## ğŸ§  CenÃ¡rio Local (Ollama / Hardware Gratuito)

Seus custos sÃ£o **Hardware**, nÃ£o dinheiro.

1. **Llama 3 (8B) / Mistral:**
    * Suporta bem atÃ© **8.000 tokens**.
    * Se seu projeto passar disso, o modelo comeÃ§a a "esquecer" o inÃ­cio do cÃ³digo (alucinaÃ§Ã£o).
    * _SoluÃ§Ã£o:_ Use modelos com janela estendida (ex: `yarn-llama-128k`) ou quantizaÃ§Ã£o maior se tiver pouca RAM.

2. **RAM NecessÃ¡ria para Contexto:**
    * Carregar o texto na memÃ³ria Ã© barato.
    * Processar (KV Cache) na GPU Ã© caro.
    * Para 32k tokens de contexto, reserve ~2GB a 4GB de VRAM extra alÃ©m do peso do modelo.

---

## ğŸ›¡ï¸ TÃ©cnicas de "Custo Zero" (Boas PrÃ¡ticas)

Para maximizar a eficiÃªncia sem gastar um centavo extra:

### 1. A Regra do `ignore_folder`

Nunca envie lixo para a IA.

```python
# No chat, execute uma vez:
ignore_folder("node_modules")  # PadrÃ£o
ignore_folder("dist")          # Build files
ignore_folder("coverage")      # RelatÃ³rios de teste
ignore_folder("assets")        # Imagens/SVGs (se nÃ£o for analisar UI)
ignore_folder("scrapes")       # Dados brutos
```

### 2. Fluxo "Ãrvore -> Folha" (Tree-to-Leaf)

Para projetos grandes, nÃ£o dÃª o cÃ³digo todo. DÃª o mapa.

1. **UsuÃ¡rio:** "Olhe a estrutura: `@project/tree`"
    * _Custo:_ ~200 tokens (Quase zero).
2. **UsuÃ¡rio:** "O erro estÃ¡ na autenticaÃ§Ã£o."
3. **IA:** "Entendi a estrutura. Por favor, leia o arquivo `src/auth/login.ts`."
4. **UsuÃ¡rio:** O sistema lÃª apenas esse arquivo.

### 3. Cache de Contexto (Context Caching)

Se usar APIs da Anthropic (Claude):

* Ative o `prompt caching`.
* Envie o `PROJECT_CONTEXT_SUMMARY.txt` como um bloco de cache.
* VocÃª pagarÃ¡ pelo envio **apenas na primeira vez**. As mensagens seguintes custam 10% do valor normal para ler o mesmo contexto.

---

## ğŸ† Veredito Final

O **Infinite Memory MCP** Ã© uma "Ferrari": extremamente rÃ¡pida e poderosa, mas consome mais combustÃ­vel.

* Use na cidade (projetos pequenos/mÃ©dios) Ã  vontade.
* Na estrada longa (projetos gigantes), dirija com inteligÃªncia (use filtros).
