# Projeto: Private AI - Mem√≥ria Infinita & Ultra-R√°pida üß†üöÄ

## Conceito

Utilizar a infraestrutura de hardware dispon√≠vel (**1TB de RAM**) para criar um Agente de IA capaz de processar contextos gigantescos com lat√™ncia zero, eliminando o gargalo de leitura de disco (I/O).

## Arquitetura Proposta

### 1. Camada de Persist√™ncia Vol√°til (RAMDisk)

Em vez de salvar o contexto em SSD, criaremos um **RAMDisk** (Disco Virtual na Mem√≥ria RAM).

* **Velocidade:** > 50 GB/s (vs 3-7 GB/s do SSD).
* **Lat√™ncia:** Nanosegundos.
* **Fun√ß√£o:** Armazenar o `PROJECT_CONTEXT_SUMMARY.txt` e a base de conhecimento vetorial.

### 2. Os Motores de Contexto (Scripts Python)

Utilizaremos vers√µes otimizadas dos scripts que j√° criamos:

#### A. O Coletor Instant√¢neo (`leitor_ram.py`)

* L√™ recursivamente o reposit√≥rio.
* Em vez de escrever no disco r√≠gido, escreve diretamente no mount point da RAM (`/mnt/ramdisk/context.txt`).
* **Melhoria:** Implementar *Watcher* de eventos de arquivo (Watchdog) para atualizar o RAMDisk apenas nos deltas (mudan√ßas), em tempo real.

#### B. O C√©rebro Local (Ollama/vLLM)

* O modelo de LLM (ex: Llama-3-70B, Mixtral 8x7B) √© carregado inteiramente na RAM.
* O Agente consulta o arquivo de contexto na RAMDisk.
* **Resultado:** O Agente "sabe" tudo sobre o projeto instantaneamente, sem delay de carregamento de contexto.

## Implementa√ß√£o (Passos Iniciais)

### Passo 1: Criar RAMDisk (Linux/WSL)

```bash
# Cria ponto de montagem
sudo mkdir /mnt/ram_context

# Monta 10GB de RAM como disco
sudo mount -t tmpfs -o size=10G tmpfs /mnt/ram_context
```

### Passo 2: Adaptar Scripts

Mover os scripts `auto_leitor.py` e `leitor_de_contexto.py` para esta pasta e configur√°-los para apontar para o `Target Path` no RAMDisk.

### Passo 3: Rodar Modelo

```bash
ollama run llama3:70b
# Configurar prompt do sistema para ler sempre de /mnt/ram_context/summary.txt
```

---
**Status:** Planejamento.
**Hardware:** 1TB RAM Dispon√≠vel.
**Objetivo:** Zero-Latency coding assistant.
